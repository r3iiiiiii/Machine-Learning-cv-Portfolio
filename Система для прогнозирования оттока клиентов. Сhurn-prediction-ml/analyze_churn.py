import pandas as pd
import numpy as np
import joblib
import warnings
warnings.filterwarnings('ignore')

import sys
sys.path.append('src')
from data_processing import DataProcessor


class ChurnAnalyzer:
    """ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ñ‚Ñ‚Ğ¾ĞºĞ° ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ’Ğ¡Ğ•Ğ¥ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°"""
    
    def __init__(self):
        self.xgb_model = None
        self.rf_model = None
        self.lr_model = None
        self.scaler = None
        self.processor = DataProcessor()
    
    def load_models(self, models_path):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"""
        models_dict = joblib.load(models_path)
        self.xgb_model = models_dict.get('XGBoost')
        self.rf_model = models_dict.get('Random Forest')
        self.lr_model = models_dict.get('Ğ›Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ')
        print("âœ… ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾!")
    
    def load_scaler(self, scaler_path):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°"""
        self.scaler = joblib.load(scaler_path)
        print("âœ… ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½!")
    
    def preprocess_new_data(self, df):
        """ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
        print("\nğŸ“¥ ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")
        
        df_processed = df.copy()
        
        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Churn ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
        if 'Churn' in df_processed.columns:
            if df_processed['Churn'].dtype == 'object':
                if df_processed['Churn'].isin(['Yes', 'No']).all():
                    df_processed['Churn'] = df_processed['Churn'].map({'Yes': 1, 'No': 0})
        
        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ TotalCharges
        if 'TotalCharges' in df_processed.columns:
            if df_processed['TotalCharges'].dtype == 'object':
                df_processed['TotalCharges'] = pd.to_numeric(df_processed['TotalCharges'], errors='coerce')
                df_processed['TotalCharges'].fillna(0, inplace=True)
        
        categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()
        
        # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ ID ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ñ‹
        id_cols = [col for col in categorical_cols if 'id' in col.lower()]
        for col in id_cols:
            if col in categorical_cols:
                categorical_cols.remove(col)
            if col in df_processed.columns:
                df_processed.drop(col, axis=1, inplace=True)
        
        # Ğ•ÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Churn, ÑƒĞ´Ğ°Ğ»ÑĞµĞ¼ ĞµĞ³Ğ¾ Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
        if 'Churn' in categorical_cols:
            categorical_cols.remove('Churn')
        
        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Yes/No Ğ² 1/0
        for col in categorical_cols:
            if df_processed[col].dtype == 'object':
                if df_processed[col].isin(['Yes', 'No']).all():
                    df_processed[col] = df_processed[col].map({'Yes': 1, 'No': 0})
        
        remaining_categorical = [col for col in categorical_cols 
                                if df_processed[col].dtype == 'object']
        
        # One-Hot Encoding
        if remaining_categorical:
            df_encoded = pd.get_dummies(df_processed, 
                                       columns=remaining_categorical, 
                                       drop_first=True)
        else:
            df_encoded = df_processed.copy()
        
        # ĞÑ‚Ğ´ĞµĞ»ÑĞµĞ¼ Churn ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
        if 'Churn' in df_encoded.columns:
            X = df_encoded.drop('Churn', axis=1)
            y = df_encoded['Churn']
        else:
            X = df_encoded
            y = None
        
        print(f"âœ… Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹! Ğ Ğ°Ğ·Ğ¼ĞµÑ€: {X.shape}")
        
        return X, y
    
    def predict(self, X):
        """ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"""
        print("\nğŸš€ ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹...\n")
        
        X_scaled = self.scaler.transform(X)
        
        # XGBoost
        try:
            xgb_pred = self.xgb_model.predict(X_scaled)
            xgb_proba = self.xgb_model.predict_proba(X_scaled)[:, 1]
            print("âœ… XGBoost Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹")
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° XGBoost: {e}")
            xgb_pred = np.zeros(len(X_scaled))
            xgb_proba = np.zeros(len(X_scaled))
        
        # Random Forest
        try:
            rf_pred = self.rf_model.predict(X_scaled)
            rf_proba = self.rf_model.predict_proba(X_scaled)[:, 1]
            print("âœ… Random Forest Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹")
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Random Forest: {e}")
            rf_pred = np.zeros(len(X_scaled))
            rf_proba = np.zeros(len(X_scaled))
        
        # Logistic Regression
        try:
            lr_pred = self.lr_model.predict(X_scaled)
            lr_proba = self.lr_model.predict_proba(X_scaled)[:, 1]
            print("âœ… Ğ›Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ»Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹")
        except Exception as e:
            print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ›Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸: {e}")
            lr_pred = np.zeros(len(X_scaled))
            lr_proba = np.zeros(len(X_scaled))
        
        # ĞĞ½ÑĞ°Ğ¼Ğ±Ğ»ÑŒ
        ensemble_proba = (xgb_proba + rf_proba + lr_proba) / 3
        ensemble_pred = (ensemble_proba >= 0.5).astype(int)
        
        results = {
            'XGBoost_pred': xgb_pred,
            'XGBoost_proba': xgb_proba,
            'RandomForest_pred': rf_pred,
            'RandomForest_proba': rf_proba,
            'LogisticRegression_pred': lr_pred,
            'LogisticRegression_proba': lr_proba,
            'Ensemble_pred': ensemble_pred,
            'Ensemble_proba': ensemble_proba
        }
        
        return results
    
    def save_predictions_csv(self, df, predictions, output_path):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² CSV"""
        print("\nğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ CSV Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²...\n")
        
        result_df = df.copy()
        
        result_df['XGBoost_ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ·'] = ['Ğ£Ğ¥ĞĞ”Ğ˜Ğ¢' if p == 1 else 'ĞĞ¡Ğ¢ĞĞĞ¢Ğ¡Ğ¯' for p in predictions['XGBoost_pred']]
        result_df['XGBoost_Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ'] = (predictions['XGBoost_proba'] * 100).round(2)
        
        result_df['RandomForest_ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ·'] = ['Ğ£Ğ¥ĞĞ”Ğ˜Ğ¢' if p == 1 else 'ĞĞ¡Ğ¢ĞĞĞ¢Ğ¡Ğ¯' for p in predictions['RandomForest_pred']]
        result_df['RandomForest_Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ'] = (predictions['RandomForest_proba'] * 100).round(2)
        
        result_df['LogisticRegression_ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ·'] = ['Ğ£Ğ¥ĞĞ”Ğ˜Ğ¢' if p == 1 else 'ĞĞ¡Ğ¢ĞĞĞ¢Ğ¡Ğ¯' for p in predictions['LogisticRegression_pred']]
        result_df['LogisticRegression_Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ'] = (predictions['LogisticRegression_proba'] * 100).round(2)
        
        result_df['Ensemble_ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ·'] = ['Ğ£Ğ¥ĞĞ”Ğ˜Ğ¢' if p == 1 else 'ĞĞ¡Ğ¢ĞĞĞ¢Ğ¡Ğ¯' for p in predictions['Ensemble_pred']]
        result_df['Ensemble_Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ'] = (predictions['Ensemble_proba'] * 100).round(2)
        
        result_df.to_csv(output_path, index=False, encoding='utf-8')
        print(f"âœ… CSV ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½: {output_path}")
        
        return result_df
    
    def save_text_report(self, df, predictions, output_path):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°"""
        print(f"ğŸ“„ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°...")
        
        ensemble_churn = (predictions['Ensemble_pred'] == 1).sum()
        ensemble_stay = (predictions['Ensemble_pred'] == 0).sum()
        avg_risk = predictions['Ensemble_proba'].mean() * 100
        
        critical = (predictions['Ensemble_proba'] >= 0.90).sum()
        high = ((predictions['Ensemble_proba'] >= 0.70) & (predictions['Ensemble_proba'] < 0.90)).sum()
        medium = ((predictions['Ensemble_proba'] >= 0.50) & (predictions['Ensemble_proba'] < 0.70)).sum()
        low = (predictions['Ensemble_proba'] < 0.50).sum()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("="*100 + "\n")
            f.write("ĞĞ¢Ğ§ĞĞ¢ Ğ ĞŸĞ ĞĞ“ĞĞĞ—Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ˜ ĞĞ¢Ğ¢ĞĞšĞ ĞšĞ›Ğ˜Ğ•ĞĞ¢ĞĞ’\n")
            f.write("="*100 + "\n\n")
            
            # ĞĞ±Ñ‰Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
            f.write("ğŸ“Š ĞĞ‘Ğ©ĞĞ¯ Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ:\n")
            f.write("-"*100 + "\n")
            f.write(f"Ğ’ÑĞµĞ³Ğ¾ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾: {len(df)}\n")
            f.write(f"ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ğ£Ğ¥ĞĞ”Ğ¯Ğ©Ğ˜Ğ¥: {ensemble_churn} ({ensemble_churn/len(df)*100:.1f}%)\n")
            f.write(f"ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ğ’Ğ•Ğ ĞĞ«Ğ¥: {ensemble_stay} ({ensemble_stay/len(df)*100:.1f}%)\n")
            f.write(f"Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ€Ğ¸ÑĞº ÑƒÑ…Ğ¾Ğ´Ğ°: {avg_risk:.2f}%\n\n")
            
            # Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ€Ğ¸ÑĞºÑƒ
            f.write("ğŸ“‹ Ğ ĞĞ¡ĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ•ĞĞ˜Ğ• ĞŸĞ ĞšĞĞ¢Ğ•Ğ“ĞĞ Ğ˜Ğ¯Ğœ Ğ Ğ˜Ğ¡ĞšĞ:\n")
            f.write("-"*100 + "\n")
            f.write(f"ğŸ”´ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ (>90%):    {critical:6d} ({critical/len(df)*100:5.1f}%) â† Ğ¡Ğ ĞĞ§ĞĞ Ğ¢Ğ Ğ•Ğ‘Ğ£Ğ®Ğ¢ Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ¯!\n")
            f.write(f"ğŸŸ  Ğ’Ğ«Ğ¡ĞĞšĞ˜Ğ™ (70-90%):      {high:6d} ({high/len(df)*100:5.1f}%) â† Ğ’Ğ«Ğ¡ĞĞšĞ˜Ğ™ ĞŸĞ Ğ˜ĞĞ Ğ˜Ğ¢Ğ•Ğ¢\n")
            f.write(f"ğŸŸ¡ Ğ¡Ğ Ğ•Ğ”ĞĞ˜Ğ™ (50-70%):      {medium:6d} ({medium/len(df)*100:5.1f}%) â† Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•\n")
            f.write(f"ğŸŸ¢ ĞĞ˜Ğ—ĞšĞ˜Ğ™ (<50%):         {low:6d} ({low/len(df)*100:5.1f}%) â† Ğ¡Ğ¢ĞĞ‘Ğ˜Ğ›Ğ¬ĞĞ«\n\n")
            
            # Ğ¢Ğ¾Ğ¿-100 Ñ€Ğ¸ÑĞºĞ¾Ğ²
            f.write("\n" + "="*100 + "\n")
            f.write("ğŸ”´ Ğ¢ĞĞŸ-100 ĞšĞ›Ğ˜Ğ•ĞĞ¢ĞĞ’ Ğ¡ ĞĞĞ˜Ğ’Ğ«Ğ¡Ğ¨Ğ˜Ğœ Ğ Ğ˜Ğ¡ĞšĞĞœ Ğ£Ğ¥ĞĞ”Ğ:\n")
            f.write("="*100 + "\n\n")
            
            sorted_indices = np.argsort(predictions['Ensemble_proba'])[::-1][:min(100, len(df))]
            
            for i, idx in enumerate(sorted_indices, 1):
                customer_id = df.iloc[idx].get('customerID', f'ID_{idx}')
                risk = predictions['Ensemble_proba'][idx] * 100
                xgb_risk = predictions['XGBoost_proba'][idx] * 100
                rf_risk = predictions['RandomForest_proba'][idx] * 100
                lr_risk = predictions['LogisticRegression_proba'][idx] * 100
                
                # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğµ (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)
                gender = df.iloc[idx].get('gender', 'N/A')
                tenure = df.iloc[idx].get('tenure', 'N/A')
                monthly = df.iloc[idx].get('MonthlyCharges', 'N/A')
                
                f.write(f"{i:3d}. ĞšĞ»Ğ¸ĞµĞ½Ñ‚: {customer_id:15s} | Ğ Ğ¸ÑĞº ÑƒÑ…Ğ¾Ğ´Ğ° (ĞĞ½ÑĞ°Ğ¼Ğ±Ğ»ÑŒ): {risk:6.2f}%\n")
                f.write(f"     ĞŸĞ¾Ğ»: {str(gender):8s} | Ğ¡Ñ‚Ğ°Ğ¶ (Ğ¼ĞµÑ): {str(tenure):6s} | ĞŸĞ»Ğ°Ñ‚Ñ‘Ğ¶/Ğ¼ĞµÑ: {str(monthly):8s}\n")
                f.write(f"     XGBoost: {xgb_risk:6.2f}% | Random Forest: {rf_risk:6.2f}% | Ğ›Ğ¾Ğ³Ñ€ĞµĞ³: {lr_risk:6.2f}%\n")
                
                # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ†Ğ²ĞµÑ‚ Ñ€Ğ¸ÑĞºĞ°
                if risk >= 90:
                    risk_level = "ğŸ”´ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ - Ğ¡Ğ ĞĞ§ĞĞ!"
                elif risk >= 70:
                    risk_level = "ğŸŸ  Ğ’Ğ«Ğ¡ĞĞšĞ˜Ğ™ - ĞŸĞ Ğ˜ĞĞ Ğ˜Ğ¢Ğ•Ğ¢"
                elif risk >= 50:
                    risk_level = "ğŸŸ¡ Ğ¡Ğ Ğ•Ğ”ĞĞ˜Ğ™ - Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•"
                else:
                    risk_level = "ğŸŸ¢ ĞĞ˜Ğ—ĞšĞ˜Ğ™ - Ğ¡Ğ¢ĞĞ‘Ğ˜Ğ›Ğ•Ğ"
                
                f.write(f"     Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ: {risk_level}\n\n")
        
        print(f"âœ… Ğ¢ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½: {output_path}")
    
    def print_console_report(self, df, predictions, total_clients):
        """ĞšÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ Ğ² ĞºĞ¾Ğ½ÑĞ¾Ğ»ÑŒ"""
        ensemble_churn = (predictions['Ensemble_pred'] == 1).sum()
        ensemble_stay = (predictions['Ensemble_pred'] == 0).sum()
        
        critical = (predictions['Ensemble_proba'] >= 0.90).sum()
        high = ((predictions['Ensemble_proba'] >= 0.70) & (predictions['Ensemble_proba'] < 0.90)).sum()
        medium = ((predictions['Ensemble_proba'] >= 0.50) & (predictions['Ensemble_proba'] < 0.70)).sum()
        low = (predictions['Ensemble_proba'] < 0.50).sum()
        
        print("\n" + "="*100)
        print("ğŸ“Š Ğ˜Ğ¢ĞĞ“ĞĞ’ĞĞ¯ Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ ĞĞĞĞ›Ğ˜Ğ—Ğ:")
        print("="*100)
        print(f"\nâœ… Ğ’ÑĞµĞ³Ğ¾ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾: {total_clients}")
        print(f"\nğŸ“ˆ ĞŸĞ ĞĞ“ĞĞĞ—Ğ« ĞĞĞ¡ĞĞœĞ‘Ğ›Ğ¯:")
        print(f"  ğŸ”´ Ğ£Ğ¥ĞĞ”Ğ¯Ğ©Ğ˜Ğ•:  {ensemble_churn:6d} ({ensemble_churn/total_clients*100:6.1f}%)")
        print(f"  ğŸŸ¢ Ğ’Ğ•Ğ ĞĞ«Ğ•:    {ensemble_stay:6d} ({ensemble_stay/total_clients*100:6.1f}%)")
        
        print(f"\nğŸ“‹ Ğ ĞĞ¡ĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ•ĞĞ˜Ğ• ĞŸĞ ĞšĞĞ¢Ğ•Ğ“ĞĞ Ğ˜Ğ¯Ğœ Ğ Ğ˜Ğ¡ĞšĞ:")
        print(f"  ğŸ”´ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ (>90%):   {critical:6d} ({critical/total_clients*100:6.1f}%) â† Ğ¡Ğ ĞĞ§ĞĞ«Ğ• ĞœĞ•Ğ Ğ«!")
        print(f"  ğŸŸ  Ğ’Ğ«Ğ¡ĞĞšĞ˜Ğ™ (70-90%):     {high:6d} ({high/total_clients*100:6.1f}%) â† Ğ’Ğ«Ğ¡ĞĞšĞ˜Ğ™ ĞŸĞ Ğ˜ĞĞ Ğ˜Ğ¢Ğ•Ğ¢")
        print(f"  ğŸŸ¡ Ğ¡Ğ Ğ•Ğ”ĞĞ˜Ğ™ (50-70%):     {medium:6d} ({medium/total_clients*100:6.1f}%) â† Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•")
        print(f"  ğŸŸ¢ ĞĞ˜Ğ—ĞšĞ˜Ğ™ (<50%):        {low:6d} ({low/total_clients*100:6.1f}%) â† Ğ¡Ğ¢ĞĞ‘Ğ˜Ğ›Ğ¬ĞĞ«")
        
        print("\n" + "-"*100)
        print("Ğ¢ĞĞŸ-20 ĞšĞ›Ğ˜Ğ•ĞĞ¢ĞĞ’ Ğ¡ ĞĞĞ˜Ğ’Ğ«Ğ¡Ğ¨Ğ˜Ğœ Ğ Ğ˜Ğ¡ĞšĞĞœ:")
        print("-"*100)
        
        sorted_indices = np.argsort(predictions['Ensemble_proba'])[::-1][:20]
        
        for i, idx in enumerate(sorted_indices, 1):
            customer_id = df.iloc[idx].get('customerID', f'ID_{idx}')
            risk = predictions['Ensemble_proba'][idx] * 100
            
            print(f"{i:2d}. {customer_id:15s} | Ğ Ğ¸ÑĞº: {risk:6.2f}% | XGB: {predictions['XGBoost_proba'][idx]*100:6.2f}% | "
                  f"RF: {predictions['RandomForest_proba'][idx]*100:6.2f}% | LR: {predictions['LogisticRegression_proba'][idx]*100:6.2f}%")


def analyze_full_dataset():
    """ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ’Ğ¡Ğ•Ğ¥ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°"""
    print("="*100)
    print("Ğ¡Ğ˜Ğ¡Ğ¢Ğ•ĞœĞ ĞŸĞ ĞĞ“ĞĞĞ—Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ¯ ĞĞ¢Ğ¢ĞĞšĞ ĞšĞ›Ğ˜Ğ•ĞĞ¢ĞĞ’ - ĞŸĞĞ›ĞĞ«Ğ™ ĞĞĞĞ›Ğ˜Ğ— Ğ”ĞĞ¢ĞĞ¡Ğ•Ğ¢Ğ")
    print("="*100)
    
    analyzer = ChurnAnalyzer()
    
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    print("\nğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹...")
    analyzer.load_models('results/Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.pkl')
    analyzer.load_scaler('results/Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€.pkl')
    
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ’Ğ¡Ğ• ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²
    print(f"ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ’Ğ¡Ğ•Ğ¥ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· data/clients.csv...")
    df = pd.read_csv('data/clients.csv')
    total_clients = len(df)
    print(f"âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {total_clients} ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²")
    
    # ĞŸÑ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°
    X, y = analyzer.preprocess_new_data(df)
    
    # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ
    predictions = analyzer.predict(X)
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ
    print("\n" + "="*100)
    print("ğŸ’¾ Ğ¡ĞĞ¥Ğ ĞĞĞ•ĞĞ˜Ğ• Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢ĞĞ’:")
    print("="*100)
    
    # CSV
    analyzer.save_predictions_csv(df, predictions, 'results/predictions.csv')
    
    # Ğ¢ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚
    analyzer.save_text_report(df, predictions, 'results/churn_report.txt')
    
    # ĞšĞ¾Ğ½ÑĞ¾Ğ»ÑŒ
    analyzer.print_console_report(df, predictions, total_clients)
    
    print("\n" + "="*100)
    print("âœ… ĞĞĞĞ›Ğ˜Ğ— Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•Ğ!")
    print("="*100)
    print(f"\nğŸ“ CSV Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ²:     results/predictions.csv")
    print(f"ğŸ“„ Ğ¢ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½ Ğ²:    results/churn_report.txt")
    print(f"\nğŸ’¡ ĞÑ‚ĞºÑ€Ğ¾Ğ¹Ñ‚Ğµ ÑÑ‚Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°!\n")


if __name__ == '__main__':
    analyze_full_dataset()
