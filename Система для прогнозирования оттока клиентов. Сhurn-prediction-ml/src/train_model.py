import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import joblib
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc
import xgboost as xgb
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings('ignore')

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
import sys
sys.path.append('src')
from data_processing import DataProcessorx


class ModelTrainer:
    """–û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.xgb_model = None
        self.rf_model = None
        self.lr_model = None
        self.ensemble_model = None
        self.metrics = {}
    
    def train_xgboost_tuned(self, X_train, y_train):
        """–û–±—É—á–µ–Ω–∏–µ XGBoost —Å hyperparameter tuning"""
        print("\n=== –û–ë–£–ß–ï–ù–ò–ï XGBoost (—Å hyperparameter tuning) ===")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        param_grid = {
            'max_depth': [5, 7, 9],
            'learning_rate': [0.01, 0.05, 0.1],
            'n_estimators': [100, 150, 200],
            'subsample': [0.8, 0.9],
            'colsample_bytree': [0.8, 0.9]
        }
        
        # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
        xgb_base = xgb.XGBClassifier(
            random_state=42,
            n_jobs=-1,
            use_label_encoder=False,
            eval_metric='logloss'
        )
        
        # GridSearchCV –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        print("–ò—â—É –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
        grid_search = GridSearchCV(
            xgb_base,
            param_grid,
            cv=3,
            scoring='roc_auc',
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(X_train, y_train)
        
        print(f"‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞–π–¥–µ–Ω—ã!")
        print(f"–õ—É—á—à–∏–π AUC: {grid_search.best_score_:.4f}")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
        
        self.xgb_model = grid_search.best_estimator_
        return self.xgb_model
    
    def train_random_forest_tuned(self, X_train, y_train):
        """–û–±—É—á–µ–Ω–∏–µ Random Forest —Å hyperparameter tuning"""
        print("\n=== –û–ë–£–ß–ï–ù–ò–ï Random Forest (—Å hyperparameter tuning) ===")
        
        param_grid = {
            'n_estimators': [100, 150, 200],
            'max_depth': [10, 15, 20],
            'min_samples_split': [5, 10],
            'min_samples_leaf': [2, 4]
        }
        
        rf_base = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight='balanced')
        
        print("–ò—â—É –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
        grid_search = GridSearchCV(
            rf_base,
            param_grid,
            cv=3,
            scoring='roc_auc',
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(X_train, y_train)
        
        print(f"‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞–π–¥–µ–Ω—ã!")
        print(f"–õ—É—á—à–∏–π AUC: {grid_search.best_score_:.4f}")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
        
        self.rf_model = grid_search.best_estimator_
        return self.rf_model
    
    def train_logistic_regression_tuned(self, X_train, y_train):
        """–û–±—É—á–µ–Ω–∏–µ Logistic Regression —Å hyperparameter tuning"""
        print("\n=== –û–ë–£–ß–ï–ù–ò–ï –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (—Å hyperparameter tuning) ===")
        
        param_grid = {
            'C': [0.001, 0.01, 0.1, 1],
            'class_weight': ['balanced', None]
        }
        
        lr_base = LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1)
        
        print("–ò—â—É –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã...")
        grid_search = GridSearchCV(
            lr_base,
            param_grid,
            cv=3,
            scoring='roc_auc',
            n_jobs=-1,
            verbose=1
        )
        
        grid_search.fit(X_train, y_train)
        
        print(f"‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞–π–¥–µ–Ω—ã!")
        print(f"–õ—É—á—à–∏–π AUC: {grid_search.best_score_:.4f}")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
        
        self.lr_model = grid_search.best_estimator_
        return self.lr_model
    
    def evaluate_models(self, models_dict, X_test, y_test):
        """–û—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        print("\n=== –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ï–ô ===\n")
        
        results = {}
        predictions = {}
        
        for name, model in models_dict.items():
            print(f"{name}:")
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)[:, 1]
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred)
            recall = recall_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred)
            auc_roc = roc_auc_score(y_test, y_pred_proba)
            
            print(f"  –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")
            print(f"  –¢–æ—á–Ω–æ—Å—Ç—å (–¥–ª—è –∫–ª–∞—Å—Å–∞ 1): {precision:.4f}")
            print(f"  –ü–æ–ª–Ω–æ—Ç–∞: {recall:.4f}")
            print(f"  F1-Score: {f1:.4f}")
            print(f"  AUC-ROC: {auc_roc:.4f}\n")
            
            results[name] = {
                'Accuracy': accuracy,
                'Precision': precision,
                'Recall': recall,
                'F1-Score': f1,
                'AUC-ROC': auc_roc
            }
            
            predictions[name] = y_pred_proba
            self.metrics[name] = results[name]
        
        return results, predictions
    
    def plot_roc_curves(self, models_dict, predictions, X_test, y_test):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ ROC –∫—Ä–∏–≤—ã—Ö"""
        print("=== –°–û–ó–î–ê–ù–ò–ï ROC –ö–†–ò–í–´–• ===")
        
        plt.figure(figsize=(10, 8))
        
        for name, model in models_dict.items():
            y_pred_proba = predictions[name]
            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
            roc_auc = auc(fpr, tpr)
            
            plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC={roc_auc:.4f})')
        
        # –î–∏–∞–≥–æ–Ω–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è
        plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='–°–ª—É—á–∞–π–Ω–æ–µ —É–≥–∞–¥—ã–≤–∞–Ω–∏–µ')
        
        plt.xlabel('False Positive Rate (–õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è)', fontsize=12)
        plt.ylabel('True Positive Rate (–í–µ—Ä–Ω–æ –Ω–∞–π–¥–µ–Ω–æ)', fontsize=12)
        plt.title('ROC –ö—Ä–∏–≤—ã–µ - –í—Å–µ –º–æ–¥–µ–ª–∏', fontsize=14, fontweight='bold')
        plt.legend(loc='lower right', fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('results/roc_–∫—Ä–∏–≤—ã–µ.png', dpi=300, bbox_inches='tight')
        print("‚úÖ ROC –∫—Ä–∏–≤—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: results/roc_–∫—Ä–∏–≤—ã–µ.png\n")
        plt.close()
    
    def plot_feature_importance(self, model, feature_names):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print("=== –°–û–ó–î–ê–ù–ò–ï –ì–†–ê–§–ò–ö–ê –í–ê–ñ–ù–û–°–¢–ò ===")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –∏–∑ XGBoost
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            indices = np.argsort(importances)[::-1][:15]  # –¢–æ–ø 15
            
            plt.figure(figsize=(12, 6))
            plt.title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (XGBoost)', fontsize=14, fontweight='bold')
            plt.bar(range(len(indices)), importances[indices], align='center')
            plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=45, ha='right')
            plt.ylabel('–í–∞–∂–Ω–æ—Å—Ç—å', fontsize=12)
            plt.xlabel('–ü—Ä–∏–∑–Ω–∞–∫–∏', fontsize=12)
            plt.tight_layout()
            plt.savefig('results/–≤–∞–∂–Ω–æ—Å—Ç—å_–ø—Ä–∏–∑–Ω–∞–∫–æ–≤.png', dpi=300, bbox_inches='tight')
            print("‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: results/–≤–∞–∂–Ω–æ—Å—Ç—å_–ø—Ä–∏–∑–Ω–∞–∫–æ–≤.png\n")
            plt.close()
    
    def save_results(self, models_dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        joblib.dump(models_dict, 'results/–º–æ–¥–µ–ª–∏.pkl')
        print("‚úÖ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: results/–º–æ–¥–µ–ª–∏.pkl")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ JSON
        with open('results/–º–µ—Ç—Ä–∏–∫–∏.json', 'w', encoding='utf-8') as f:
            json.dump(self.metrics, f, ensure_ascii=False, indent=2)
        print("‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: results/–º–µ—Ç—Ä–∏–∫–∏.json")


def apply_smote(X_train, y_train):
    """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤"""
    print("\n=== –ü–†–ò–ú–ï–ù–ï–ù–ò–ï SMOTE (–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤) ===")
    
    print(f"–î–û SMOTE:")
    print(f"  –ö–ª–∞—Å—Å 0: {(y_train == 0).sum()} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"  –ö–ª–∞—Å—Å 1: {(y_train == 1).sum()} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    smote = SMOTE(random_state=42, k_neighbors=5)
    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
    
    print(f"\n–ü–û–°–õ–ï SMOTE:")
    print(f"  –ö–ª–∞—Å—Å 0: {(y_train_smote == 0).sum()} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"  –ö–ª–∞—Å—Å 1: {(y_train_smote == 1).sum()} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"‚úÖ –ö–ª–∞—Å—Å—ã —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã!\n")
    
    return X_train_smote, y_train_smote


def apply_feature_engineering(X_train, X_test):
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("=== FEATURE ENGINEERING (—Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤) ===")
    
    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è train
    X_train_fe = X_train.copy()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º
    if 'MonthlyCharges' in X_train.columns and 'TotalCharges' in X_train.columns:
        X_train_fe['monthly_to_total'] = X_train['MonthlyCharges'] / (X_train['TotalCharges'] + 1)
        print("‚úÖ –°–æ–∑–¥–∞–Ω –ø—Ä–∏–∑–Ω–∞–∫: monthly_to_total")
    
    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è test (—Å —Ç–µ–º–∏ –∂–µ —Ñ—É–Ω–∫—Ü–∏—è–º–∏)
    X_test_fe = X_test.copy()
    if 'MonthlyCharges' in X_test.columns and 'TotalCharges' in X_test.columns:
        X_test_fe['monthly_to_total'] = X_test['MonthlyCharges'] / (X_test['TotalCharges'] + 1)
    
    print(f"‚úÖ Feature Engineering –∑–∞–≤–µ—Ä—à—ë–Ω! –ù–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_fe.shape[1]}\n")
    
    return X_train_fe, X_test_fe


def apply_cross_validation(model, X_train, y_train):
    """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ cross-validation"""
    print("=== CROSS-VALIDATION (5-fold CV) ===")
    
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc', n_jobs=-1)
    
    print(f"CV AUC scores: {cv_scores}")
    print(f"–°—Ä–µ–¥–Ω–∏–π AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
    print(f"‚úÖ Cross-validation –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\n")
    
    return cv_scores


def main():
    print("=" * 70)
    print("–°–ò–°–¢–ï–ú–ê –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –û–¢–¢–û–ö–ê –ö–õ–ò–ï–ù–¢–û–í (OPTIMIZED VERSION)")
    print("=" * 70)
    
    # ========== –ó–ê–ì–†–£–ó–ö–ê –ò –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ==========
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    processor = DataProcessor()
    df = processor.load_data('data/clients.csv')
    
    print(f"\nDataset shape: {df.shape}")
    processor.analyze_data(df)
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    X_train, X_test, y_train, y_test = processor.preprocess_data(df)
    
    # ========== FEATURE ENGINEERING ==========
    # –ù—É–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –æ–±—Ä–∞—Ç–Ω–æ –≤ DataFrame –¥–ª—è feature engineering
    # (—Ç.–∫. –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ preprocessing - —ç—Ç–æ numpy arrays)
    # –ü–æ–∫–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –º–æ–∂–µ—Ç —É—Å–ª–æ–∂–Ω–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å
    
    # ========== –ü–†–ò–ú–ï–ù–ï–ù–ò–ï SMOTE ==========
    X_train_smote, y_train_smote = apply_smote(X_train, y_train)
    
    # ========== –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ==========
    print("\nüöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å hyperparameter tuning...\n")
    
    trainer = ModelTrainer()
    
    # –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–∏—Å–∫–æ–º –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    xgb_model = trainer.train_xgboost_tuned(X_train_smote, y_train_smote)
    rf_model = trainer.train_random_forest_tuned(X_train_smote, y_train_smote)
    lr_model = trainer.train_logistic_regression_tuned(X_train_smote, y_train_smote)
    
    # ========== CROSS-VALIDATION ==========
    print("\nüìä –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π...\n")
    
    cv_xgb = apply_cross_validation(xgb_model, X_train_smote, y_train_smote)
    cv_rf = apply_cross_validation(rf_model, X_train_smote, y_train_smote)
    cv_lr = apply_cross_validation(lr_model, X_train_smote, y_train_smote)
    
    # ========== –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ï–ô ==========
    models_dict = {
        'XGBoost': xgb_model,
        'Random Forest': rf_model,
        '–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è': lr_model
    }
    
    results, predictions = trainer.evaluate_models(models_dict, X_test, y_test)
    
    # ========== –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ==========
    print("üìà –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...\n")
    
    trainer.plot_roc_curves(models_dict, predictions, X_test, y_test)
    trainer.plot_feature_importance(xgb_model, processor.feature_names)
    
    # ========== –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ==========
    trainer.save_results(models_dict)
    processor.save_scaler('results/–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä.pkl')
    
    # ========== –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ ==========
    print("\n" + "=" * 70)
    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 70)
    
    print("\nüìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print("  ‚úì results/–º–æ–¥–µ–ª–∏.pkl - –û–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏")
    print("  ‚úì results/–º–µ—Ç—Ä–∏–∫–∏.json - –¢–∞–±–ª–∏—Ü–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏")
    print("  ‚úì results/–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä.pkl - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö")
    print("  ‚úì results/–≤–∞–∂–Ω–æ—Å—Ç—å_–ø—Ä–∏–∑–Ω–∞–∫–æ–≤.png - –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏")
    print("  ‚úì results/roc_–∫—Ä–∏–≤—ã–µ.png - ROC –∫—Ä–∏–≤—ã–µ")
    
    print("\nüéØ –ú–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ (–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è):")
    best_model_metrics = trainer.metrics['–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è']
    for metric, value in best_model_metrics.items():
        print(f"  {metric}: {value:.4f}")
    
    print("\nüöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print("  1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∏ –≤ –ø–∞–ø–∫–µ results/")
    print("  2. –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ results/–º–µ—Ç—Ä–∏–∫–∏.json")
    print("  3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–æ–≤—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤")
    print("  4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python analyze_churn.py –î–õ–Ø –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤")


if __name__ == '__main__':
    main()